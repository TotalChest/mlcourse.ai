{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: Юрий Кашницкий\n",
    "    \n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейные модели классификации и регрессии\n",
    "## <center>Часть 2. Логистическая регрессия и метод максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"../../img/logit.png\">\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^{\\text{T}}\\textbf x),$$\n",
    "\n",
    "где\n",
    " - $\\textbf{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\textbf{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\textbf{x})$ – ответ классификатора на примере $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\textbf{x}_\\text{i}$ к классу \"+\":\n",
    "$$\\Large p_+ = \\text P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (пока считаем, что веса $\\textbf{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к принципу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ называется *отступом* (*margin*) классификации на объекте $\\textbf{x}_\\text{i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\textbf{x}_\\text{i}$, если же отрицателен – значит, класс для $\\textbf{x}_\\text{i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\textbf{x}_A$ до плоскости, которая задается уравнением $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "\n",
    "Ответ: \n",
    "$\\Large \\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$, тем дальше точка $\\textbf{x}_\\text{i}$ находится от плоскости $\\textbf{w}^{\\text{T}}\\textbf{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\textbf{x}_\\text{i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяющей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\textbf{y}$ у выборки $\\textbf X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $\\textbf X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в данном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\Large \\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w})$ – попросту число ошибок логистической регрессии с весами $\\textbf{w}$ на выборке $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{\\log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{\\log}} (X, \\textbf{y}, \\textbf{w})$ минимизируется следующий:\n",
    "\n",
    "$$\\Large J(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\Large \\widehat{\\textbf{w}}  = \\arg \\min_{\\textbf{w}} J(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
